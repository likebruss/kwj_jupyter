{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Symbol', 'Price', 'Date', 'Time', 'Change', 'Volume']\n",
      "['AA', '39.48', '6/11/2007', '9:36am', '-0.18', '181800']\n",
      "['AIG', '71.38', '6/11/2007', '9:36am', '-0.15', '195500']\n",
      "['AXP', '62.58', '6/11/2007', '9:36am', '-0.46', '935000']\n",
      "['BA', '98.31', '6/11/2007', '9:36am', '+0.12', '104800']\n",
      "['C', '53.08', '6/11/2007', '9:36am', '-0.25', '360900']\n",
      "['CAT', '78.29', '6/11/2007', '9:36am', '-0.23', '225400']\n",
      "<class 'list'>\n",
      "Change :  -0.23\n"
     ]
    }
   ],
   "source": [
    "# 6.1. Reading and Writing CSV Data\n",
    "# Read CSV rows as lists\n",
    "import csv\n",
    "with open('files/stocks.csv') as f:\n",
    "    f_csv = csv.reader(f)\n",
    "    headers = next(f_csv)\n",
    "    print(headers)\n",
    "    for row in f_csv:\n",
    "        print(row)\n",
    "print(type(row))\n",
    "print(headers[4],': ',row[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Symbol='AA', Price='39.48', Date='6/11/2007', Time='9:36am', Change='-0.18', Volume='181800')\n",
      "Row(Symbol='AIG', Price='71.38', Date='6/11/2007', Time='9:36am', Change='-0.15', Volume='195500')\n",
      "Row(Symbol='AXP', Price='62.58', Date='6/11/2007', Time='9:36am', Change='-0.46', Volume='935000')\n",
      "Row(Symbol='BA', Price='98.31', Date='6/11/2007', Time='9:36am', Change='+0.12', Volume='104800')\n",
      "Row(Symbol='C', Price='53.08', Date='6/11/2007', Time='9:36am', Change='-0.25', Volume='360900')\n",
      "Row(Symbol='CAT', Price='78.29', Date='6/11/2007', Time='9:36am', Change='-0.23', Volume='225400')\n",
      "CAT\n"
     ]
    }
   ],
   "source": [
    "# 6.1.\n",
    "# Read CSV rows as namedtules (the column headers are valid Python identifiers)\n",
    "from collections import namedtuple\n",
    "with open('files/stocks.csv') as f:\n",
    "    f_csv = csv.reader(f)\n",
    "    headings = next(f_csv)\n",
    "    Row = namedtuple('Row', headings)\n",
    "    for r in f_csv:\n",
    "        row = Row(*r)\n",
    "        print(row)\n",
    "print(row.Symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Symbol\\tPrice\\tDate\\tTime\\tChange\\tVolume': 'AA\\t39.48\\t\"6/11/2007\"\\t\"9:36am\"\\t-0.18\\t181800'}\n",
      "{'Symbol\\tPrice\\tDate\\tTime\\tChange\\tVolume': 'AIG\\t71.38\\t\"6/11/2007\"\\t\"9:36am\"\\t-0.15\\t195500'}\n",
      "{'Symbol\\tPrice\\tDate\\tTime\\tChange\\tVolume': 'AXP\\t62.58\\t\"6/11/2007\"\\t\"9:36am\"\\t-0.46\\t935000'}\n",
      "{'Symbol\\tPrice\\tDate\\tTime\\tChange\\tVolume': 'BA\\t98.31\\t\"6/11/2007\"\\t\"9:36am\"\\t+0.12\\t104800'}\n",
      "{'Symbol\\tPrice\\tDate\\tTime\\tChange\\tVolume': 'C\\t53.08\\t\"6/11/2007\"\\t\"9:36am\"\\t-0.25\\t360900'}\n",
      "{'Symbol\\tPrice\\tDate\\tTime\\tChange\\tVolume': 'CAT\\t78.29\\t\"6/11/2007\"\\t\"9:36am\"\\t-0.23\\t225400'}\n"
     ]
    }
   ],
   "source": [
    "# 6.1.\n",
    "# Read CSV rows as dictionaries\n",
    "import csv\n",
    "with open('files/stocks.tsv') as f:\n",
    "    f_csv = csv.DictReader(f)\n",
    "    for row in f_csv:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Symbol', 'Price', 'Date', 'Time', 'Change', 'Volume']\n",
      "['AA', '39.48', '6/11/2007', '9:36am', '-0.18', '181800']\n",
      "['AIG', '71.38', '6/11/2007', '9:36am', '-0.15', '195500']\n",
      "['AXP', '62.58', '6/11/2007', '9:36am', '-0.46', '935000']\n",
      "['BA', '98.31', '6/11/2007', '9:36am', '+0.12', '104800']\n",
      "['C', '53.08', '6/11/2007', '9:36am', '-0.25', '360900']\n",
      "['CAT', '78.29', '6/11/2007', '9:36am', '-0.23', '225400']\n"
     ]
    }
   ],
   "source": [
    "# 6.1.\n",
    "# Example of reading tab-separated values\n",
    "with open('files/stocks.tsv') as f:\n",
    "    f_tsv = csv.reader(f, delimiter='\\t')\n",
    "    for row in f_tsv:\n",
    "            print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 6.1.\n",
    "# write lists to CSV file\n",
    "headers = ['Symbol','Price','Date','Time','Change','Volume']\n",
    "rows = [('AA', 39.48, '6/11/2007', '9:36am', -0.18, 181800),\n",
    "        ('AIG', 71.38, '6/11/2007', '9:36am', -0.15, 195500),\n",
    "        ('AXP', 62.58, '6/11/2007', '9:36am', -0.46, 935000),\n",
    "        ]\n",
    "with open('files/stocks.csv','w') as f:\n",
    "    f_csv = csv.writer(f)\n",
    "    f_csv.writerow(headers)\n",
    "    f_csv.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 6.1.\n",
    "# write dictionaries to CSV file\n",
    "headers = ['Symbol', 'Price', 'Date', 'Time', 'Change', 'Volume']\n",
    "rows = [{'Symbol':'AA', 'Price':39.48, 'Date':'6/11/2007',\n",
    "        'Time':'9:36am', 'Change':-0.18, 'Volume':181800},\n",
    "        {'Symbol':'AIG', 'Price': 71.38, 'Date':'6/11/2007',\n",
    "        'Time':'9:36am', 'Change':-0.15, 'Volume': 195500},\n",
    "        {'Symbol':'AXP', 'Price': 62.58, 'Date':'6/11/2007',\n",
    "        'Time':'9:36am', 'Change':-0.46, 'Volume': 935000},\n",
    "        ]\n",
    "with open('stocks.csv','w') as f:\n",
    "    f_csv = csv.DictWriter(f, headers)\n",
    "    f_csv.writeheader()\n",
    "    f_csv.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Symbol='AA', Price='39.48', Date='6/11/2007', Time='9:36am', Change='-0.18', Volume='181800')\n",
      "Row(Symbol='AIG', Price='71.38', Date='6/11/2007', Time='9:36am', Change='-0.15', Volume='195500')\n",
      "Row(Symbol='AXP', Price='62.58', Date='6/11/2007', Time='9:36am', Change='-0.46', Volume='935000')\n"
     ]
    }
   ],
   "source": [
    "# 6.1.\n",
    "# Read CSV rows with nonvalid Python identifiers\n",
    "import re\n",
    "with open('files/stocks.csv') as f:\n",
    "    f_csv = csv.reader(f)\n",
    "    headers = [ re.sub('[^a-zA-Z_]', '_', h) for h in next(f_csv) ]\n",
    "    Row = namedtuple('Row', headers)\n",
    "    for r in f_csv:\n",
    "        row = Row(*r)\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('AA', 39.48, '6/11/2007', '9:36am', -0.18, 181800)\n",
      "('AIG', 71.38, '6/11/2007', '9:36am', -0.15, 195500)\n",
      "('AXP', 62.58, '6/11/2007', '9:36am', -0.46, 935000)\n",
      "\n",
      "{'Price': 39.48, 'Symbol': 'AA', 'Time': '9:36am', 'Change': -0.18, 'Date': '6/11/2007', 'Volume': 181800}\n",
      "{'Price': 71.38, 'Symbol': 'AIG', 'Time': '9:36am', 'Change': -0.15, 'Date': '6/11/2007', 'Volume': 195500}\n",
      "{'Price': 62.58, 'Symbol': 'AXP', 'Time': '9:36am', 'Change': -0.46, 'Date': '6/11/2007', 'Volume': 935000}\n"
     ]
    }
   ],
   "source": [
    "# 6.1.\n",
    "# Read CSV rows with data type conversions\n",
    "col_types = [str, float, str, str, float, int]\n",
    "with open('files/stocks.csv') as f:\n",
    "    f_csv = csv.reader(f)\n",
    "    headers = next(f_csv)\n",
    "    for row in f_csv:\n",
    "        # Apply conversions to the row items\n",
    "        row = tuple(convert(value) for convert, value in zip(col_types, row))\n",
    "        print(row)\n",
    "print()\n",
    "\n",
    "# Reading as dicts with type conversion\n",
    "field_types = [ ('Price', float),\n",
    "                ('Change', float),\n",
    "                ('Volume', int) ]\n",
    "with open('files/stocks.csv') as f:\n",
    "    for row in csv.DictReader(f):\n",
    "        row.update((key, conversion(row[key])) for key, conversion in field_types)\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'price': 542.23, 'shares': 100, 'name': 'ACME'}\n",
      "{\"price\": 542.23, \"shares\": 100, \"name\": \"ACME\"}\n",
      "{'price': 542.23, 'shares': 100, 'name': 'ACME'}\n"
     ]
    }
   ],
   "source": [
    "# 6.2. Reading and Writing JSON Data\n",
    "import json\n",
    "data = {\n",
    "    'name' : 'ACME',\n",
    "    'shares' : 100,\n",
    "    'price' : 542.23\n",
    "    }\n",
    "json_str = json.dumps(data)\n",
    "print(data)\n",
    "print(json_str)\n",
    "\n",
    "# Writing JSON data\n",
    "with open('files/data.json', 'w') as f:\n",
    "    json.dump(data, f)\n",
    "\n",
    "# Reading data back\n",
    "with open('files/data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"a\": true, \"b\": false, \"c\": null}\n",
      "\n",
      "{'display': 'standalone',\n",
      " 'gcm_sender_id': '211794955562',\n",
      " 'gcm_user_visible_only': True,\n",
      " 'name': 'Onet.pl',\n",
      " 'short_name': 'Onet'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6.2. \n",
    "# JSON mapping\n",
    "d= {'a': True, 'b': False, 'c': None}\n",
    "print(json.dumps(d))\n",
    "print()\n",
    "\n",
    "# JSON from website:\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "u = urlopen('https://www.onet.pl/manifest.json')\n",
    "resp = json.loads(u.read().decode('utf-8'))\n",
    "from pprint import pprint\n",
    "pprint(resp)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'ACME'), ('shares', 50), ('price', 490.1)])\n",
      "\n",
      "name: ACME \tshares: 50 \tprice: 490.1\n",
      "\n",
      "\"{\\\"name\\\": \\\"ACME\\\", \\\"shares\\\": 50, \\\"price\\\": 490.1}\"\n",
      "\"{\\\"name\\\": \\\"ACME\\\", \\\"shares\\\": 50, \\\"price\\\": 490.1}\"\n",
      "\"{\\\"name\\\": \\\"ACME\\\", \\\"shares\\\": 50, \\\"price\\\": 490.1}\"\n"
     ]
    }
   ],
   "source": [
    "# 6.2.\n",
    "# decode JSON data, preserving its order in an OrderedDict:\n",
    "s=  '{\"name\": \"ACME\", \"shares\": 50, \"price\": 490.1}'\n",
    "from collections import OrderedDict\n",
    "data = json.loads(s, object_pairs_hook=OrderedDict)\n",
    "print(data)\n",
    "print()\n",
    "\n",
    "# turn a JSON dictionary into a Python object:\n",
    "class JSONObject:\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d\n",
    "data = json.loads(s, object_hook=JSONObject)\n",
    "print('name:', data.name, '\\tshares:', data.shares,'\\tprice:', data.price)\n",
    "print()\n",
    "\n",
    "# output nicely formatted - somtethingnot working for variable: data ???\n",
    "print(json.dumps(s))\n",
    "print(json.dumps(s, indent=4))\n",
    "print(json.dumps(s, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d: {'__classname__': 'Point'}\n",
      "d: {'__classname__': 'Point', 'y': 3, 'x': 2}\n",
      "serialise: {\"__classname__\": \"Point\", \"y\": 3, \"x\": 2}\n",
      "clsname: Point\n",
      "key y value 3\n",
      "key x value 2\n",
      "a.x: 2 a.y: 3\n"
     ]
    }
   ],
   "source": [
    "# 6.2.\n",
    "# (c) Encoding instances\n",
    "class Point:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "def serialize_instance(obj):\n",
    "    d = { '__classname__' : type(obj).__name__ }\n",
    "    print('d:',d)\n",
    "    d.update(vars(obj))\n",
    "    print('d:',d)\n",
    "    return d\n",
    "\n",
    "p = Point(2,3)\n",
    "s = json.dumps(p, default=serialize_instance)\n",
    "print(\"serialise:\",s)\n",
    "\n",
    "# (d) Decoding instances\n",
    "classes = {\n",
    "    'Point' : Point\n",
    "}\n",
    "\n",
    "def unserialize_object(d):\n",
    "    clsname = d.pop('__classname__', None)\n",
    "    print('clsname:', clsname)\n",
    "    if clsname:\n",
    "        cls = classes[clsname]\n",
    "        obj = cls.__new__(cls)\n",
    "        for key, value in d.items():\n",
    "            setattr(obj, key, value)\n",
    "            print('key', key,'value', value)\n",
    "        return obj\n",
    "    else:\n",
    "        return d\n",
    "\n",
    "a = json.loads(s, object_hook=unserialize_object)\n",
    "print('a.x:',a.x,'a.y:',a.y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc: <xml.etree.ElementTree.ElementTree object at 0x7f8930145eb8>\n",
      "e: <Element 'title' at 0x7f89300eadb8>\n",
      "e.tag: title e.text: Planet Python\n",
      "None\n",
      "--------------------\n",
      "Possbility and Probability: Debugging Flask, requests, curl, and form data\n",
      "Mon, 28 Aug 2017 14:09:40 +0000\n",
      "https://ironboundsoftware.com/blog/2017/08/28/debugging-flask-requests-curl-and-form-data/\n",
      "\n",
      "Mike Driscoll: Back to School Python Book Sale 2017\n",
      "Mon, 28 Aug 2017 13:52:54 +0000\n",
      "http://www.blog.pythonlibrary.org/2017/08/28/back-to-school-python-book-sale-2017/\n",
      "\n",
      "Chris Moffitt: Building a Bullet Graph in Python\n",
      "Mon, 28 Aug 2017 13:38:00 +0000\n",
      "http://pbpython.com/bullet-graph.html\n",
      "\n",
      "Doug Hellmann: smtplib — Simple Mail Transfer Protocol Client — PyMOTW 3\n",
      "Mon, 28 Aug 2017 13:00:40 +0000\n",
      "http://feeds.doughellmann.com/~r/doughellmann/python/~3/pE7PsR_Vd1c/\n",
      "\n",
      "Mike Driscoll: PyDev of the Week: Shannon Turner\n",
      "Mon, 28 Aug 2017 12:30:36 +0000\n",
      "http://www.blog.pythonlibrary.org/2017/08/28/pydev-of-the-week-shannon-turner/\n",
      "\n",
      "Import Python: Import Python 140 -  Publish your Python packages, Python for research course, sys.getrefcount ...\n",
      "Mon, 28 Aug 2017 07:13:15 +0000\n",
      "http://importpython.com/blog/post/import-python-140-publish-your-python-packages-python-research-course-sysgetrefcount\n",
      "\n",
      "Continuum Analytics Blog: Continuum Analytics Officially Becomes Anaconda\n",
      "Mon, 28 Aug 2017 02:35:27 +0000\n",
      "https://www.anaconda.com/company-blog/continuum-analytics-officially-becomes-anaconda/\n",
      "\n",
      "Carl Chenet: The Importance of Choosing the Correct Mastodon Instance\n",
      "Sun, 27 Aug 2017 22:00:46 +0000\n",
      "https://carlchenet.com/the-importance-of-choosing-the-correct-mastodon-instance/\n",
      "\n",
      "Jaime Buelta: Notes about ShipItCon 2017\n",
      "Sun, 27 Aug 2017 17:43:50 +0000\n",
      "https://wrongsideofmemphis.wordpress.com/2017/08/27/notes-about-shipitcon-2017/\n",
      "\n",
      "Django Weekly: DjangoWeekly 53 - Celery Workflow, Transaction Hooks, Django Rest API\n",
      "Sun, 27 Aug 2017 08:00:54 +0000\n",
      "http://djangoweekly.com/blog/post/djangoweekly-53-celery-workflow-transaction-hooks-django-rest-api\n",
      "\n",
      "Codementor: Building a desktop notification tool using python\n",
      "Sun, 27 Aug 2017 07:02:27 +0000\n",
      "https://www.codementor.io/dushyantbgs/building-a-desktop-notification-tool-using-python-bcpya9cwh\n",
      "\n",
      "Python Data: Forecasting Time Series data with Prophet – Jupyter Notebook\n",
      "Sun, 27 Aug 2017 01:01:27 +0000\n",
      "http://pythondata.com/forecasting-time-series-data-prophet-jupyter-notebook/\n",
      "\n",
      "Python Insider: Python 2.7.14 release candidate 1 available\n",
      "Sat, 26 Aug 2017 23:41:40 +0000\n",
      "http://feedproxy.google.com/~r/PythonInsider/~3/pe2Ug4MA0Lg/python-2714-release-candidate-1.html\n",
      "\n",
      "Zato Blog: Zato 2.0.8 released\n",
      "Sat, 26 Aug 2017 22:00:00 +0000\n",
      "https://zato.io/blog/posts/zato-2.0.8-released.html\n",
      "\n",
      "Weekly Python StackOverflow Report: (lxxxviii) stackoverflow python report\n",
      "Sat, 26 Aug 2017 19:27:00 +0000\n",
      "http://python-weekly.blogspot.com/2017/08/lxxxviii-stackoverflow-python-report.html\n",
      "\n",
      "Stein Magnus Jodal: Bringing the Mopidy music server to the browser\n",
      "Sat, 26 Aug 2017 00:00:00 +0000\n",
      "https://jodal.no/2017/08/26/mopidy-in-the-browser/\n",
      "\n",
      "Mauveweb: Fun and Games in Python (Pycon PL Keynote)\n",
      "Fri, 25 Aug 2017 21:52:00 +0000\n",
      "http://mauveweb.co.uk/posts/2017/08/fun-and-games.html\n",
      "\n",
      "Evennia: Renaming Django's Auth User and App\n",
      "Fri, 25 Aug 2017 21:39:28 +0000\n",
      "http://evennia.blogspot.com/2017/08/renaming-djangos-auth-user-and-app.html\n",
      "\n",
      "بايثون العربي: كيفية التحقق من وجود الملفات في بايثون\n",
      "Fri, 25 Aug 2017 21:12:02 +0000\n",
      "http://pyarab.com/2017/08/python-check-if-file-exists.html\n",
      "\n",
      "The Three of Wands: attrs II: Slots\n",
      "Fri, 25 Aug 2017 20:29:00 +0000\n",
      "https://threeofwands.com/attrs-ii-slots/\n",
      "\n",
      "Python Engineering at Microsoft: Python updates in Visual Studio 15.4 Preview 1\n",
      "Fri, 25 Aug 2017 20:00:31 +0000\n",
      "https://blogs.msdn.microsoft.com/pythonengineering/2017/08/25/python-updates-in-visual-studio-15-4-preview-1/\n",
      "\n",
      "Sandipan Dey: Diffusion, PDE and Variational Methods in Image Processing and Computer Vision (Python implementation)\n",
      "Thu, 24 Aug 2017 22:47:43 +0000\n",
      "\n",
      "\n",
      "Python Does What?!: sqlite does what\n",
      "Thu, 24 Aug 2017 17:31:30 +0000\n",
      "http://www.pythondoeswhat.com/2017/08/sqlite-does-what.html\n",
      "\n",
      "Continuum Analytics News: Continuum Analytics Welcomes Mathew Lodge as SVP Products and Marketing\n",
      "Thu, 24 Aug 2017 15:56:12 +0000\n",
      "https://www.continuum.io/blog/news/continuum-analytics-welcomes-mathew-lodge-svp-products-and-marketing\n",
      "\n",
      "PyCharm: PyCharm 2017.2.2 is now available\n",
      "Thu, 24 Aug 2017 14:54:18 +0000\n",
      "http://feedproxy.google.com/~r/Pycharm/~3/zH-Ct6uaPi4/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6.3. Parsing Simple XML Data\n",
    "from urllib.request import urlopen\n",
    "from xml.etree.ElementTree import parse\n",
    "# Download the RSS feed and parse it\n",
    "u = urlopen('http://planet.python.org/rss20.xml')\n",
    "doc = parse(u)\n",
    "print('doc:', doc)\n",
    "e = doc.find('channel/title')\n",
    "print('e:', e)\n",
    "print('e.tag:', e.tag, 'e.text:', e.text)\n",
    "print(e.get('some_attribute'))\n",
    "print('-'*20)\n",
    "\n",
    "# Extract and output tags of interest\n",
    "for item in doc.iterfind('channel/item'):\n",
    "    title = item.findtext('title')\n",
    "    date = item.findtext('pubDate')\n",
    "    link = item.findtext('link')\n",
    "    print(title)\n",
    "    print(date)\n",
    "    print(link)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60617 13\n",
      "60626 8\n",
      "60651 7\n",
      "60623 6\n",
      "60647 6\n",
      "60613 4\n",
      "60625 4\n",
      "60636 4\n",
      "60609 4\n",
      "60628 4\n",
      "60622 3\n",
      "60641 3\n",
      "60619 3\n",
      "60629 3\n",
      "60657 3\n",
      "60649 2\n",
      "60618 2\n",
      "60644 2\n",
      "60638 2\n",
      "60656 2\n",
      "60654 2\n",
      "60652 1\n",
      "60634 1\n",
      "60643 1\n",
      "60612 1\n",
      "60631 1\n",
      "60614 1\n",
      "60707 1\n",
      "60632 1\n",
      "60616 1\n",
      "60630 1\n",
      "60639 1\n",
      "60660 1\n",
      "60637 1\n"
     ]
    }
   ],
   "source": [
    "# 6.4. Parsing Huge XML Files Incrementally\n",
    "from xml.etree.ElementTree import iterparse\n",
    "def parse_and_remove(filename, path):\n",
    "    path_parts = path.split('/')\n",
    "    doc = iterparse(filename, ('start', 'end'))\n",
    "    # Skip the root element\n",
    "    next(doc)\n",
    "\n",
    "    tag_stack = []\n",
    "    elem_stack = []\n",
    "    for event, elem in doc:\n",
    "        if event == 'start':\n",
    "            tag_stack.append(elem.tag)\n",
    "            elem_stack.append(elem)\n",
    "        elif event == 'end':\n",
    "            if tag_stack == path_parts:\n",
    "                yield elem\n",
    "                elem_stack[-2].remove(elem)\n",
    "            try:\n",
    "                tag_stack.pop()\n",
    "                elem_stack.pop()\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "# Find zip code with most potholes\n",
    "\n",
    "from collections import Counter\n",
    "potholes_by_zip = Counter()\n",
    "\n",
    "data = parse_and_remove('files/potholes.xml', 'row/row')\n",
    "for pothole in data:\n",
    "    potholes_by_zip[pothole.findtext('zip')] += 1\n",
    "\n",
    "for zipcode, num in potholes_by_zip.most_common():\n",
    "    print(zipcode, num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<stock><price>490.1</price><shares>100</shares><name>GOOG</name></stock>'\n",
      "b'<stock _id=\"1234\"><price>490.1</price><shares>100</shares><name>GOOG</name></stock>'\n"
     ]
    }
   ],
   "source": [
    "# 6.5. Turning a Dictionary into XML\n",
    "from xml.etree.ElementTree import Element\n",
    "def dict_to_xml(tag, d):\n",
    "    ''' Turn a simple dict of key/value pairs into XML '''\n",
    "    elem = Element(tag)\n",
    "    for key, val in d.items():\n",
    "        child = Element(key)\n",
    "        child.text = str(val)\n",
    "        elem.append(child)\n",
    "    return elem\n",
    "s = { 'name': 'GOOG', 'shares': 100, 'price':490.1 }\n",
    "e = dict_to_xml('stock', s)\n",
    "from xml.etree.ElementTree import tostring\n",
    "print(tostring(e))\n",
    "e.set('_id','1234') # attach attributes to an element\n",
    "print(tostring(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<item><name><spam></name></item>\n",
      "b'<item><name>&lt;spam&gt;</name></item>'\n",
      "\n",
      "&lt;spam&gt;\n",
      "<spam>\n"
     ]
    }
   ],
   "source": [
    "# 6.5. \n",
    "def dict_to_xml_str(tag, d):\n",
    "    ''' Turn a simple dict of key/value pairs into XML '''\n",
    "    parts = ['<{}>'.format(tag)]\n",
    "    for key, val in d.items():\n",
    "        parts.append('<{0}>{1}</{0}>'.format(key,val))\n",
    "    parts.append('</{}>'.format(tag))\n",
    "    return ''.join(parts)\n",
    "d = { 'name' : '<spam>' }\n",
    "print(dict_to_xml_str('item',d))   # String creation -> BUG\n",
    "\n",
    "# Proper XML creation\n",
    "e = dict_to_xml('item',d)\n",
    "print(tostring(e))\n",
    "print()\n",
    "\n",
    "from xml.sax.saxutils import escape, unescape\n",
    "print(escape('<spam>'))\n",
    "print(unescape('<spam>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Element 'stop' at 0x7f8930102098>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# 6.6. Parsing, Modifying, and Rewriting XML\n",
    "from xml.etree.ElementTree import parse, Element\n",
    "doc = parse('files/pred.xml')\n",
    "root = doc.getroot()\n",
    "print(root)\n",
    "# Remove a few elements\n",
    "root.remove(root.find('sri'))\n",
    "root.remove(root.find('cr'))\n",
    "\n",
    "# Insert a new element after <nm>...</nm>\n",
    "nm_index = root.getchildren().index(root.find('nm'))\n",
    "print(nm_index)\n",
    "\n",
    "e = Element('spam')\n",
    "e.text = 'This is a test'\n",
    "root.insert(nm_index + 1, e)\n",
    "\n",
    "# Write back to a file\n",
    "doc.write('files/newpred.xml', xml_declaration=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "David Beazley\n",
      "<Element 'content' at 0x7ffa54318a48>\n",
      "None\n",
      "<Element '{http://www.w3.org/1999/xhtml}html' at 0x7ffa54318b38>\n",
      "None\n",
      "Hello World\n",
      "to są połączone teksty\n"
     ]
    }
   ],
   "source": [
    "# 6.7. Parsing XML Documents with Namespaces\n",
    "from xml.etree.ElementTree import parse, Element\n",
    "doc= parse('files/author.xml')\n",
    "# Some queries that work\n",
    "print(doc.findtext('author'))\n",
    "print(doc.find('content'))\n",
    "# A query involving a namespace (doesn't work)\n",
    "print(doc.find('content/html'))\n",
    "# Works if fully qualified\n",
    "print(doc.find('content/{http://www.w3.org/1999/xhtml}html'))\n",
    "# Doesn't work\n",
    "print(doc.findtext('content/{http://www.w3.org/1999/xhtml}html/head/title'))\n",
    "# Fully qualified\n",
    "print(doc.findtext('content/{http://www.w3.org/1999/xhtml}html/'\n",
    "                   '{http://www.w3.org/1999/xhtml}head/'\n",
    "                   '{http://www.w3.org/1999/xhtml}title'))\n",
    "print('to są ''połączone teksty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item.name: html item.uri: http://www.w3.org/1999/xhtml\n",
      "namespaces: {'html': '{http://www.w3.org/1999/xhtml}'}\n",
      "\n",
      "format_map: content/{http://www.w3.org/1999/xhtml}html\n",
      "<Element '{http://www.w3.org/1999/xhtml}html' at 0x7ffa54318b38>\n",
      "\n",
      "format_map: content/{http://www.w3.org/1999/xhtml}html/{http://www.w3.org/1999/xhtml}head/{http://www.w3.org/1999/xhtml}title\n",
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "# 6.7.\n",
    "# wrapping namespace\n",
    "class XMLNamespaces:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.namespaces = {}\n",
    "        for name, uri in kwargs.items():\n",
    "            self.register(name, uri)\n",
    "            print('item.name:', name, 'item.uri:', uri)\n",
    "        print('namespaces:', self.namespaces)\n",
    "    def register(self, name, uri):\n",
    "        self.namespaces[name] = '{'+uri+'}'\n",
    "    def __call__(self, path):\n",
    "        print('format_map:', path.format_map(self.namespaces))\n",
    "        return path.format_map(self.namespaces)\n",
    "ns = XMLNamespaces(html='http://www.w3.org/1999/xhtml')\n",
    "print()\n",
    "print(doc.find(ns('content/{html}html')))\n",
    "print()\n",
    "print(doc.findtext(ns('content/{html}html/{html}head/{html}title')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end <Element 'author' at 0x7ffa5430bc28>\n",
      "start-ns ('', 'http://www.w3.org/1999/xhtml')\n",
      "end <Element '{http://www.w3.org/1999/xhtml}title' at 0x7ffa542b19f8>\n",
      "end <Element '{http://www.w3.org/1999/xhtml}head' at 0x7ffa542b16d8>\n",
      "end <Element '{http://www.w3.org/1999/xhtml}h1' at 0x7ffa542b1228>\n",
      "end <Element '{http://www.w3.org/1999/xhtml}body' at 0x7ffa542b1138>\n",
      "end <Element '{http://www.w3.org/1999/xhtml}html' at 0x7ffa542b17c8>\n",
      "end-ns None\n",
      "end <Element 'content' at 0x7ffa5430bd68>\n",
      "end <Element 'top' at 0x7ffa5430bb38>\n",
      "<Element 'top' at 0x7ffa5430bb38>\n"
     ]
    }
   ],
   "source": [
    "# 6.7.\n",
    "from xml.etree.ElementTree import iterparse\n",
    "for evt, elem in iterparse('files/author.xml', ('end', 'start-ns', 'end-ns')): print(evt, elem)\n",
    "\n",
    "print(elem)   # This is the topmost element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('GOOG', 100, 490.1)\n",
      "('AAPL', 50, 545.75)\n",
      "('FB', 150, 7.45)\n",
      "('HPQ', 75, 33.2)\n"
     ]
    }
   ],
   "source": [
    "# 6.8. Interacting with a Relational Database\n",
    "stocks = [\n",
    "    ('GOOG', 100, 490.1),\n",
    "    ('AAPL', 50, 545.75),\n",
    "    ('FB', 150, 7.45),\n",
    "    ('HPQ', 75, 33.2),\n",
    "    ]\n",
    "import sqlite3\n",
    "# connect to the database\n",
    "db = sqlite3.connect('files/database.db')\n",
    "# create a cursor\n",
    "c = db.cursor()\n",
    "c.execute('create table portfolio (symbol text, shares integer, price real)')\n",
    "db.commit()\n",
    "# insert a sequence of rows into the data\n",
    "c.executemany('insert into portfolio values (?,?,?)', stocks)\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('GOOG', 100, 490.1)\n",
      "('AAPL', 50, 545.75)\n",
      "('FB', 150, 7.45)\n",
      "('HPQ', 75, 33.2)\n",
      "\n",
      "('GOOG', 100, 490.1)\n",
      "('AAPL', 50, 545.75)\n"
     ]
    }
   ],
   "source": [
    "# 6.8.\n",
    "# perform a query\n",
    "for row in db.execute('select * from portfolio'): print(row)\n",
    "print()\n",
    "min_price = 100\n",
    "for row in db.execute('select * from portfolio where price >= ?', (min_price,)): print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'68656c6c6f'\n",
      "b'hello'\n",
      "\n",
      "b'68656C6C6F'\n",
      "68656C6C6F\n",
      "b'hello'\n"
     ]
    }
   ],
   "source": [
    "# 6.9. Decoding and Encoding Hexadecimal Digits\n",
    "# Initial byte string\n",
    "s = b'hello'\n",
    "# Encode as hex\n",
    "import binascii\n",
    "h = binascii.b2a_hex(s)\n",
    "print(h)\n",
    "# Decode back to bytes\n",
    "print(binascii.a2b_hex(h))\n",
    "print()\n",
    "\n",
    "import base64\n",
    "h = base64.b16encode(s)\n",
    "print(h)\n",
    "print(h.decode('ascii'))\n",
    "print(base64.b16decode(h))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'aGVsbG8='\n",
      "aGVsbG8=\n",
      "b'hello'\n"
     ]
    }
   ],
   "source": [
    "# 6.10. Decoding and Encoding Base64\n",
    "# Some byte data\n",
    "s = b'hello'\n",
    "import base64\n",
    "# Encode as Base64\n",
    "a = base64.b64encode(s)\n",
    "print(a)\n",
    "print(a.decode('ascii'))\n",
    "# Decode from Base64\n",
    "print(base64.b64decode(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2.3, 4.5)\n",
      "1 2.3 4.5\n",
      "(6, 7.8, 9.0)\n",
      "6 7.8 9.0\n",
      "(12, 13.4, 56.7)\n",
      "12 13.4 56.7\n"
     ]
    }
   ],
   "source": [
    "# 6.11. Reading and Writing Binary Arrays of Structures\n",
    "from struct import Struct\n",
    "def write_records(records, format, f):\n",
    "    ''' Write a sequence of tuples to a binary file of structures. '''\n",
    "    record_struct = Struct(format)\n",
    "    for r in records:\n",
    "        print(r)\n",
    "        print(*r)\n",
    "        f.write(record_struct.pack(*r))\n",
    "records = [ (1, 2.3, 4.5),\n",
    "            (6, 7.8, 9.0),\n",
    "            (12, 13.4, 56.7) ]\n",
    "with open('files/data.b', 'wb') as f:\n",
    "    write_records(records, '<idd', f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2.3, 4.5)\n",
      "(6, 7.8, 9.0)\n",
      "(12, 13.4, 56.7)\n",
      "\n",
      "(1, 2.3, 4.5)\n",
      "(6, 7.8, 9.0)\n",
      "(12, 13.4, 56.7)\n"
     ]
    }
   ],
   "source": [
    "# 6.11.\n",
    "# read the file incrementally in chunks\n",
    "from struct import Struct\n",
    "def read_records(format, f):\n",
    "    # '<idd' - Little endian 32-bit integer, two double precision floats\n",
    "    record_struct = Struct(format)\n",
    "    chunks = iter(lambda: f.read(record_struct.size), b'')\n",
    "    return (record_struct.unpack(chunk) for chunk in chunks)\n",
    "with open('files/data.b','rb') as f:\n",
    "    for rec in read_records('<idd', f):\n",
    "        print(rec)\n",
    "print()\n",
    "\n",
    "# read the file entirely \n",
    "def unpack_records(format, data):\n",
    "    record_struct = Struct(format)\n",
    "    return (record_struct.unpack_from(data, offset) \n",
    "            for offset in range(0, len(data), record_struct.size))\n",
    "with open('files/data.b', 'rb') as f:\n",
    "    data = f.read()\n",
    "    for rec in unpack_records('<idd', data):\n",
    "        print(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Struct object at 0x7ffa543c4810>\n",
      "20\n",
      "b'\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00@\\x00\\x00\\x00\\x00\\x00\\x00\\x08@'\n",
      "(1, 2.0, 3.0)\n",
      "b'\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00@\\x00\\x00\\x00\\x00\\x00\\x00\\x08@'\n",
      "(1, 2.0, 3.0)\n"
     ]
    }
   ],
   "source": [
    "# 6.11.\n",
    "from struct import Struct\n",
    "record_struct = Struct('<idd')\n",
    "print(record_struct)\n",
    "print(record_struct.size)\n",
    "print(record_struct.pack(1, 2.0, 3.0))\n",
    "print(record_struct.unpack(_))\n",
    "import struct\n",
    "print(struct.pack('<idd', 1, 2.0, 3.0))\n",
    "print(struct.unpack('<idd', _))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x01\\x00\\x00\\x00ffffff\\x02@\\x00\\x00\\x00\\x00\\x00\\x00\\x12@'\n",
      "b'\\x06\\x00\\x00\\x00333333\\x1f@\\x00\\x00\\x00\\x00\\x00\\x00\"@'\n",
      "b'\\x0c\\x00\\x00\\x00\\xcd\\xcc\\xcc\\xcc\\xcc\\xcc*@\\x9a\\x99\\x99\\x99\\x99YL@'\n"
     ]
    }
   ],
   "source": [
    "# 6.11.\n",
    "f = open('files/data.b', 'rb')\n",
    "chunks = iter(lambda: f.read(20), b'')\n",
    "for chk in chunks:\n",
    "    print(chk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2.3 4.5\n",
      "6 7.8 9.0\n",
      "12 13.4 56.7\n"
     ]
    }
   ],
   "source": [
    "# 6.11.\n",
    "from collections import namedtuple\n",
    "Record = namedtuple('Record', ['kind','x','y'])\n",
    "with open('files/data.b', 'rb') as f:\n",
    "    records = (Record(*r) for r in read_records('<idd', f))\n",
    "    for r in records:\n",
    "        print(r.kind, r.x, r.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[( 1,   2.3,   4.5) ( 6,   7.8,   9. ) (12,  13.4,  56.7)]\n",
      "(1,  2.3,  4.5)\n"
     ]
    }
   ],
   "source": [
    "# 6.11.\n",
    "import numpy as np\n",
    "f = open('files/data.b', 'rb')\n",
    "records = np.fromfile(f, dtype='<i,<d,<d')\n",
    "print(records)\n",
    "print(records[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.0, 2.5), (3.5, 4.0), (2.5, 1.5), (7.0, 1.2), (5.1, 3.0), (0.5, 7.5), (0.8, 9.0), (3.4, 6.3), (1.2, 0.5), (4.6, 9.2)]\n",
      "\n",
      "[[(1.0, 2.5), (3.5, 4.0), (2.5, 1.5)], [(7.0, 1.2), (5.1, 3.0), (0.5, 7.5), (0.8, 9.0)], [(3.4, 6.3), (1.2, 0.5), (4.6, 9.2)]]\n"
     ]
    }
   ],
   "source": [
    "# 6.12. Reading Nested and Variable-Sized Binary Structures\n",
    "# READ IT AGAIN !!!\n",
    "import struct\n",
    "import itertools\n",
    "polys = [\n",
    "    [ (1.0, 2.5), (3.5, 4.0), (2.5, 1.5) ],\n",
    "    [ (7.0, 1.2), (5.1, 3.0), (0.5, 7.5), (0.8, 9.0) ],\n",
    "    [ (3.4, 6.3), (1.2, 0.5), (4.6, 9.2) ],\n",
    "    ]\n",
    "print(list(itertools.chain(*polys)))\n",
    "def write_polys(filename, polys):\n",
    "    # Determine bounding box\n",
    "    flattened = list(itertools.chain(*polys))\n",
    "    min_x = min(x for x, y in flattened)\n",
    "    max_x = max(x for x, y in flattened)\n",
    "    min_y = min(y for x, y in flattened)\n",
    "    max_y = max(y for x, y in flattened)\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(struct.pack('<iddddi', 0x1234, min_x, min_y, max_x, max_y,len(polys)))\n",
    "        for poly in polys:\n",
    "            size = len(poly) * struct.calcsize('<dd')\n",
    "            f.write(struct.pack('<i', size+4))\n",
    "            for pt in poly:\n",
    "                f.write(struct.pack('<dd', *pt))\n",
    "# Call it with our polygon data\n",
    "write_polys('files/polys.bin', polys)\n",
    "print()\n",
    "\n",
    "def read_polys(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        # Read the header\n",
    "        header = f.read(40)\n",
    "        file_code, min_x, min_y, max_x, max_y, num_polys = \\\n",
    "            struct.unpack('<iddddi', header)\n",
    "        polys = []\n",
    "        for n in range(num_polys):\n",
    "            pbytes, = struct.unpack('<i', f.read(4))\n",
    "            poly = []\n",
    "            for m in range(pbytes // 16):\n",
    "                pt = struct.unpack('<dd', f.read(16))\n",
    "                poly.append(pt)\n",
    "            polys.append(poly)\n",
    "    return polys\n",
    "p = read_polys('files/polys.bin')\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0.5\n",
      "0.5\n",
      "7.0\n",
      "9.2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# 6.12. \n",
    "# READ IT AGAIN !!!\n",
    "# Example 1\n",
    "import struct\n",
    "class StructField:\n",
    "    ''' Descriptor representing a simple structure field '''\n",
    "    def __init__(self, format, offset):\n",
    "        self.format = format\n",
    "        self.offset = offset\n",
    "    def __get__(self, instance, cls):\n",
    "        if instance is None:\n",
    "            return self\n",
    "        else:\n",
    "            r = struct.unpack_from(self.format, instance._buffer, self.offset)\n",
    "            return r[0] if len(r) == 1 else r\n",
    "\n",
    "class Structure:\n",
    "    def __init__(self, bytedata):\n",
    "        self._buffer = memoryview(bytedata)\n",
    "        \n",
    "class PolyHeader(Structure):\n",
    "    file_code = StructField('<i', 0)\n",
    "    min_x = StructField('<d', 4)\n",
    "    min_y = StructField('<d', 12)\n",
    "    max_x = StructField('<d', 20)\n",
    "    max_y = StructField('<d', 28)\n",
    "    num_polys = StructField('<i', 36)\n",
    "\n",
    "f = open('files/polys.bin', 'rb')\n",
    "phead = PolyHeader(f.read(40))\n",
    "print(phead.file_code == 0x1234)\n",
    "print(phead.min_x)\n",
    "print(phead.min_y)\n",
    "print(phead.max_x)\n",
    "print(phead.max_y)\n",
    "print(phead.num_polys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "min_x= 0.5\n",
      "max_x= 7.0\n",
      "min_y= 0.5\n",
      "max_y= 9.2\n",
      "num_polys= 3\n"
     ]
    }
   ],
   "source": [
    "# 6.12.\n",
    "# READ IT AGAIN !!!\n",
    "# Example 2: Introduction of a metaclass\n",
    "\n",
    "import struct\n",
    "\n",
    "class StructField:\n",
    "    def __init__(self, format, offset):\n",
    "        self.format = format\n",
    "        self.offset = offset\n",
    "    def __get__(self, instance, cls):\n",
    "        if instance is None:\n",
    "            return self\n",
    "        else:\n",
    "            r =  struct.unpack_from(self.format, instance._buffer, self.offset)\n",
    "            return r[0] if len(r) == 1 else r\n",
    "        \n",
    "class StructureMeta(type):\n",
    "    ''' Metaclass that automatically creates StructField descriptors '''\n",
    "    def __init__(self, clsname, bases, clsdict):\n",
    "        fields = getattr(self, '_fields_', [])\n",
    "        byte_order = ''\n",
    "        offset = 0\n",
    "        for format, fieldname in fields:\n",
    "            if format.startswith(('<','>','!','@')):\n",
    "                byte_order = format[0]\n",
    "                format = format[1:]\n",
    "            format = byte_order + format\n",
    "            setattr(self, fieldname, StructField(format, offset))\n",
    "            offset += struct.calcsize(format)\n",
    "        setattr(self, 'struct_size', offset)\n",
    "\n",
    "class Structure(metaclass=StructureMeta):\n",
    "    def __init__(self, bytedata):\n",
    "        self._buffer = memoryview(bytedata)\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(cls, f):\n",
    "        return cls(f.read(cls.struct_size))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    class PolyHeader(Structure):classmethod\n",
    "        _fields_ = [\n",
    "            ('<i', 'file_code'),\n",
    "            ('d', 'min_x'),\n",
    "            ('d', 'min_y'),\n",
    "            ('d', 'max_x'),\n",
    "            ('d', 'max_y'),\n",
    "            ('i', 'num_polys')\n",
    "            ]\n",
    "\n",
    "    f = open('files/polys.bin','rb')\n",
    "    phead = PolyHeader.from_file(f)\n",
    "    print(phead.file_code == 0x1234)\n",
    "    print('min_x=', phead.min_x)\n",
    "    print('max_x=', phead.max_x)\n",
    "    print('min_y=', phead.min_y)\n",
    "    print('max_y=', phead.max_y)\n",
    "    print('num_polys=', phead.num_polys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "min.x= 0.5\n",
      "max.x= 7.0\n",
      "min.y= 0.5\n",
      "max.y= 9.2\n",
      "num_polys= 3\n"
     ]
    }
   ],
   "source": [
    "# 6.12.\n",
    "# Example 3: Nested structure support\n",
    "\n",
    "import struct\n",
    "\n",
    "class StructField:\n",
    "    ''' Descriptor representing a simple structure field '''\n",
    "    def __init__(self, format, offset):\n",
    "        self.format = format\n",
    "        self.offset = offset\n",
    "    def __get__(self, instance, cls):\n",
    "        if instance is None:\n",
    "            return self\n",
    "        else:\n",
    "            r =  struct.unpack_from(self.format, instance._buffer, self.offset)\n",
    "            return r[0] if len(r) == 1 else r\n",
    "\n",
    "class NestedStruct:\n",
    "    ''' Descriptor representing a nested structure '''\n",
    "    def __init__(self, name, struct_type, offset):\n",
    "        self.name = name\n",
    "        self.struct_type = struct_type\n",
    "        self.offset = offset\n",
    "    def __get__(self, instance, cls):\n",
    "        if instance is None:\n",
    "            return self\n",
    "        else:\n",
    "            data = instance._buffer[self.offset:\n",
    "                                    self.offset+self.struct_type.struct_size]\n",
    "            result = self.struct_type(data)\n",
    "            # Save resulting structure back on instance to avoid\n",
    "            # further recomputation of this step\n",
    "            setattr(instance, self.name, result)\n",
    "            return result\n",
    "        \n",
    "class StructureMeta(type):\n",
    "    ''' Metaclass that automatically creates StructField descriptors '''\n",
    "    def __init__(self, clsname, bases, clsdict):\n",
    "        fields = getattr(self, '_fields_', [])\n",
    "        byte_order = ''\n",
    "        offset = 0\n",
    "        for format, fieldname in fields:\n",
    "            if isinstance(format, StructureMeta):\n",
    "                setattr(self, fieldname, NestedStruct(fieldname, format, offset))\n",
    "                offset += format.struct_size\n",
    "            else:\n",
    "                if format.startswith(('<','>','!','@')):\n",
    "                    byte_order = format[0]\n",
    "                    format = format[1:]\n",
    "                format = byte_order + format\n",
    "                setattr(self, fieldname, StructField(format, offset))\n",
    "                offset += struct.calcsize(format)\n",
    "        setattr(self, 'struct_size', offset)\n",
    "\n",
    "class Structure(metaclass=StructureMeta):\n",
    "    def __init__(self, bytedata):\n",
    "        self._buffer = memoryview(bytedata)\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(cls, f):\n",
    "        return cls(f.read(cls.struct_size))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    class Point(Structure):\n",
    "        _fields_ = [\n",
    "            ('<d', 'x'),\n",
    "            ('d', 'y')\n",
    "            ]\n",
    "\n",
    "    class PolyHeader(Structure):\n",
    "        _fields_ = [\n",
    "            ('<i', 'file_code'),\n",
    "            (Point, 'min'),\n",
    "            (Point, 'max'),\n",
    "            ('i', 'num_polys')\n",
    "            ]\n",
    "\n",
    "    f = open('files/polys.bin','rb')\n",
    "    phead = PolyHeader.from_file(f)\n",
    "    print(phead.file_code == 0x1234)\n",
    "    print('min.x=', phead.min.x)\n",
    "    print('max.x=', phead.max.x)\n",
    "    print('min.y=', phead.min.y)\n",
    "    print('max.y=', phead.max.y)\n",
    "    print('num_polys=', phead.num_polys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(1.0, 2.5), (3.5, 4.0), (2.5, 1.5)], [(7.0, 1.2), (5.1, 3.0), (0.5, 7.5), (0.8, 9.0)], [(3.4, 6.3), (1.2, 0.5), (4.6, 9.2)]]\n"
     ]
    }
   ],
   "source": [
    "# 6.12.\n",
    "# Example 4: Variable sized chunks\n",
    "\n",
    "import struct\n",
    "\n",
    "class StructField:\n",
    "    ''' Descriptor representing a simple structure field '''\n",
    "    def __init__(self, format, offset):\n",
    "        self.format = format\n",
    "        self.offset = offset\n",
    "    def __get__(self, instance, cls):\n",
    "        if instance is None:\n",
    "            return self\n",
    "        else:\n",
    "            r =  struct.unpack_from(self.format, \n",
    "                                    instance._buffer, self.offset)\n",
    "            return r[0] if len(r) == 1 else r\n",
    "\n",
    "class NestedStruct:\n",
    "    '''Descriptor representing a nested structure '''\n",
    "    def __init__(self, name, struct_type, offset):\n",
    "        self.name = name\n",
    "        self.struct_type = struct_type\n",
    "        self.offset = offset\n",
    "    def __get__(self, instance, cls):\n",
    "        if instance is None:\n",
    "            return self\n",
    "        else:\n",
    "            data = instance._buffer[self.offset:\n",
    "                                    self.offset+self.struct_type.struct_size]\n",
    "            result = self.struct_type(data)\n",
    "            setattr(instance, self.name, result)\n",
    "            return result\n",
    "        \n",
    "class StructureMeta(type):\n",
    "    ''' Metaclass that automatically creates StructField descriptors '''\n",
    "    def __init__(self, clsname, bases, clsdict):\n",
    "        fields = getattr(self, '_fields_', [])\n",
    "        byte_order = ''\n",
    "        offset = 0\n",
    "        for format, fieldname in fields:\n",
    "            if isinstance(format, StructureMeta):\n",
    "                setattr(self, fieldname, NestedStruct(fieldname, format, offset))\n",
    "                offset += format.struct_size\n",
    "            else:\n",
    "                if format.startswith(('<','>','!','@')):\n",
    "                    byte_order = format[0]\n",
    "                    format = format[1:]\n",
    "                format = byte_order + format\n",
    "                setattr(self, fieldname, StructField(format, offset))\n",
    "                offset += struct.calcsize(format)\n",
    "        setattr(self, 'struct_size', offset)\n",
    "\n",
    "class Structure(metaclass=StructureMeta):\n",
    "    def __init__(self, bytedata):\n",
    "        self._buffer = memoryview(bytedata)\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(cls, f):\n",
    "        return cls(f.read(cls.struct_size))\n",
    "\n",
    "class SizedRecord:\n",
    "    def __init__(self, bytedata):\n",
    "        self._buffer = memoryview(bytedata)\n",
    "        \n",
    "    @classmethod\n",
    "    def from_file(cls, f, size_fmt, includes_size=True):\n",
    "        sz_nbytes = struct.calcsize(size_fmt)\n",
    "        sz_bytes = f.read(sz_nbytes)\n",
    "        sz, = struct.unpack(size_fmt, sz_bytes)\n",
    "        buf = f.read(sz - includes_size * sz_nbytes)\n",
    "        return cls(buf)\n",
    "\n",
    "    def iter_as(self, code):\n",
    "        if isinstance(code, str):\n",
    "            s = struct.Struct(code)\n",
    "            for off in range(0, len(self._buffer), s.size):\n",
    "                yield s.unpack_from(self._buffer, off)\n",
    "        elif isinstance(code, StructureMeta):\n",
    "            size = code.struct_size\n",
    "            for off in range(0, len(self._buffer), size):\n",
    "                data = self._buffer[off:off+size]\n",
    "                yield code(data)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    class Point(Structure):\n",
    "        _fields_ = [\n",
    "            ('<d', 'x'),\n",
    "            ('d', 'y')\n",
    "            ]\n",
    "\n",
    "    class PolyHeader(Structure):\n",
    "        _fields_ = [\n",
    "            ('<i', 'file_code'),\n",
    "            (Point, 'min'),\n",
    "            (Point, 'max'),\n",
    "            ('i', 'num_polys')\n",
    "            ]\n",
    "\n",
    "    def read_polys(filename):\n",
    "        polys = []\n",
    "        with open(filename, 'rb') as f:\n",
    "            phead = PolyHeader.from_file(f)\n",
    "            for n in range(phead.num_polys):\n",
    "                rec = SizedRecord.from_file(f, '<i')\n",
    "                poly = [ (p.x, p.y)\n",
    "                         for p in rec.iter_as(Point) ]\n",
    "                polys.append(poly)\n",
    "        return polys\n",
    "\n",
    "    polys = read_polys('files/polys.bin')\n",
    "    print(polys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Current Activity  ZIP Code            value\n",
      "0            Creation Date     74055  non-null values\n",
      "1                   Status     74055  non-null values\n",
      "2          Completion Date     72154  non-null values\n",
      "3          Completion Date     72154  non-null values\n",
      "4   Service Request Number     74055  non-null values\n",
      "5  Type of Service Request     74055  non-null values\n",
      "6                 Latitude     74043  non-null values\n",
      "7                Longitude     74043  non-null values\n",
      "8                 Location     74043  non-null values\n",
      "\n",
      "Current Activity unique:\n",
      " ['Creation Date' 'Status' 'Completion Date' 'Service Request Number'\n",
      " 'Type of Service Request' 'Latitude' 'Longitude' 'Location']\n",
      "\n",
      "crew_dispatched:\n",
      "   Current Activity  ZIP Code            value\n",
      "2  Completion Date     72154  non-null values\n",
      "3  Completion Date     72154  non-null values\n",
      "\n",
      "ZIP Code:\n",
      " 74055    4\n",
      "74043    3\n",
      "72154    2\n",
      "Name: ZIP Code, dtype: int64\n",
      "\n",
      "len(dates): 8\n",
      "\n",
      "date_counts:\n",
      " Current Activity\n",
      "Completion Date            2\n",
      "Creation Date              1\n",
      "Latitude                   1\n",
      "Location                   1\n",
      "Longitude                  1\n",
      "Service Request Number     1\n",
      "Status                     1\n",
      "Type of Service Request    1\n",
      "dtype: int64\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'sort'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-28ab926563c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Sort the counts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mdate_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kwol/.local/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3081\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3083\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'sort'"
     ]
    }
   ],
   "source": [
    "# 6.13. Summarizing Data and Performing Statistics\n",
    "import pandas\n",
    "# Read a CSV file, skipping last line\n",
    "rats = pandas.read_csv('files/rats.csv', skipfooter=1, engine='python')\n",
    "print(rats)\n",
    "print()\n",
    "# Investigate range of values for a certain field\n",
    "print('Current Activity unique:\\n', rats['Current Activity'].unique())\n",
    "print()\n",
    "# Filter the data\n",
    "crew_dispatched = rats[rats['Current Activity'] == 'Completion Date']\n",
    "print('crew_dispatched:\\n', crew_dispatched)\n",
    "print()\n",
    "# Find 10 most rat-infested ZIP codes in Chicago\n",
    "print('ZIP Code:\\n', rats['ZIP Code'].value_counts()[:10])\n",
    "print()\n",
    "# Group by completion date\n",
    "dates = rats.groupby('Current Activity')\n",
    "print('len(dates):', len(dates))\n",
    "print()\n",
    "# Determine counts on each day\n",
    "date_counts = dates.size()\n",
    "print('date_counts:\\n', date_counts[0:10])\n",
    "print()\n",
    "# Sort the counts\n",
    "# date_counts.sort()\n",
    "# print(date_counts[-10:])\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
